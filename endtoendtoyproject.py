# -*- coding: utf-8 -*-
"""EndtoEndToyProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RF7uZh9Ata3Msaq7RlWl6mCFX6S-udvR

This video wass totally devoted to the overview of how to approach any ML project.

For which we are preparing a project titled End to End toy project. It's present at the video 13 of the ML playlist by Campus X

Importing necessary libraries:
"""

import numpy as np
import pandas as pd

"""Now we will read the uploaded csv file:"""

df = pd.read_csv('placement.csv')

"""Now, we will see what are the columns in this CSV file, for which we will use the .head():"""

df.head()

"""We will now see how many rows we have, it means how many datas we have, using .shape:"""

df.shape

"""Clearly there are 100 rows, it means 100 student datas we have in total. Now, how to approach:

step0: Preprocess + EDA + feature selection
step1: Extract input and output cols
step2: Scale the values
step3: Train test split (cross validation)
step4: Train the model
step5: Evaluate the model/model selection
step6: Deploy the model

Now, we will start preprocessing. But how? We will basically check if we have values for each of the rows or not.

For this we will use .info()
"""

df.info()

"""Clearly, it is visible that we have values for each row, but we have one un necessry row named "unnamed". So, we will now try to remove that row, using .iloc[range of row : range of row,range of column : range of column]"""

df = df.iloc[:,1:]

"""Now, we again do the preview of the data, using .head():"""

df.head()

"""Now, we will do the EDA (exploratory data analysis) on this data. For which we will use the matplot library:

"""

import matplotlib.pyplot as plt

"""Now, we will plot the scatter plot for this dataset, we will basically first plot the cgpa on x-axis and IQ on y-axis. For which we will use .scatter(df['x-axi_name'], df['y-axis_name'])

"""

plt.scatter(df['cgpa'], df['iq'])

"""Now, to see in graph who got placement, we will color the graph, for whih we will use the "placement" column:

"""

plt.scatter(df['cgpa'], df['iq'], c = df['placement'])

"""Now, we will do the feature selection, but since we have a limited and small dataset, we will ignore this phase.

Now, we will start the step1, extract input (independent var. ) and output (dependent var).
"""

X = df.iloc[:,0:2] #input column
y = df.iloc[:,-1] #ouput column

"""Now, print the columns and check if we selected the right columns:"""

X

y

"""Now, we will skip the step2 and move to step3, for performing step3 Train test split, we will use a library named sklearn.
First we will import that library:
"""

from sklearn.model_selection import train_test_split

"""Now, we will split the data, it means we will decide what % of data will go in training and what % will go in testing purpose.

For which we will use the function of sklearn, train_test_split(input,ouput,test_size=0.1)

Here, 0.1 specify the % of data with which we want our model to get tested, it means we want our model to get trained with 90% of our data.
"""

train_test_split(X,y,test_size=0.1)

"""Now, since the split gives us four different data set, we will then store them in four different variables."""

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)

"""Let's check the values stored in those variables. Since, values will be random.

"""

X_train

X_test

y_train

y_test

"""Now, we will go the step2 of scaling, since it's not always necessary but still we will do so.

For this, we will use one class named StandardScaler of sklearn.preprocessing, so let's first import it:
"""

from sklearn.preprocessing import StandardScaler

"""Now, we will create one object of this class, named scaler:"""

scaler = StandardScaler()

"""Now, we will scale those 4 variables, also we will fit the dataset, it means the function .fit_transform first will understand the data and then scale the data:"""

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Now, we will train the model (step4). For which we will use Logistic regression class from the sklearn. For which we will do this:"""

from sklearn.linear_model import LogisticRegression

"""Now, we will create an object which has power to access the functions which are there in logistic regression class. For this we will create object named, clf.

"""

clf = LogisticRegression()

"""Also, we will train the model using .fit(X_train,y_train):"""

clf.fit(X_train,y_train)

"""Now, we will go to step 5, that is we will now evaluate the model. This is done using the test data. For which we will use .predict function of the Logistic regression.

"""

clf.predict(X_test) #prediction by model

y_test #actual result

"""Now, we wan to compare how well our model worked, for which we can use another function accuracy_score from sklearn.metrics, it will help to comapre.

So, to import it:
"""

from sklearn.metrics import accuracy_score

"""Now, we will store the prediction by model in a variable, then we will pass it in the accuracy_score function."""

y_predict = clf.predict(X_test)

accuracy_score(y_test,y_predict)

"""Now, we will use the plot_decision_regions from mlxtend.plotting to plot the above output. As well as to get the decision boundary.

So, let's first import that:
"""

from mlxtend.plotting import plot_decision_regions

"""Now, we will convert those variables to numpy array which were created earlier, and pass it to function plot_decision.

It be like this:
"""

plot_decision_regions(X_train,y_train.values,clf=clf,legend=2)

"""Now, we will import this model. For which we will import library named 'pickle', which will convert the object into file and which can be used then."""

import pickle

"""Now, we will use this .dump to convert this clf object to file:"""

pickle.dump(clf,open('model.pkl','wb'))